---
layout: post
title: My Week in ML, 2/14/2022
tweetText: My Week in ML, 2/14/2022 - More Woodshedding
---

<h3>Woodshedding</h3>
It's getting boring in Marty Neural Net-ville.  Pleased with the results of slowing down, I've gone back and re-read Geron Chapter 10.  And while doing so, have begun building a [cheatsheet] of all NN things.  Never again will I be reading about a new concept, and realize I don't really know the fundamentals it's building upon!  My own little AI winter, if you will.

I realized (towards end of week) that if I'm going to have a "boring" week like this one, I owe it to myself (and my thirsty readers) to at least read/peruse some topics outside of my studies.  This will broaden my knowledge, help me find new paths of discovery, and make this blog slightly more interesting.  So for this week...

* [evolutionary automl]: rather than hand-craft the architecture of a NN, this research used evolutionary techniques to build successful architectures over many "generations".  Similar to how NNs throw computing power at (what is ultimately) a math problem, this does the same for the architectures of such a solution.  One of the main reasons ML continues to grow in capabilities is because each new achievement builds on the shoulders of the giants that came before it...and this feels like another layer in that march forward.

<h3>Upcoming Week</h3>
This past week involved note-taking on Chapter 10.  In the upcoming week, I'll be doing the same on Chapter 11.  I may get around to coding some examples, etc, but no guarantees (or pressure!)

[cheatsheet]: https://docs.google.com/document/d/1kGfLcnc9ytj6Eyp57ckHXRfG59y1gnEHm8REu9WURzQ/edit?usp=sharing
[evolutionary automl]: https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html
